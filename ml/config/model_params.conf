[NN]
# choose the RNN model e.g. LSTM, GRU
# see https://keras.io/layers/recurrent/
rnn: 'LSTM'

# number of dense (fully connected) layers (int)
n_layers = 2

# number of nodes in each layer (int)
layer_nodes = 70

# bool, if batch normalization should be performed
batch_norm = True

# bool if PReLU activation should be used
activation = 'relu'
# activation = 'PReLU'
# activation = 'ELU'
# activation = 'LeakyReLU'

# batch size (int)
batch_size: 16

# number of epochs (int)
n_epochs: 50

# dropout (float [0,1] or list with length n_layers)
dropout: 0.7

# the next two values are not important
# kernel regularizer (float (0,1))
k_reg: 0.01

# class weight (dictionary, e.g. {0: 1., 1: 50.} this means treat every instance of class 1 as 50 
# instances of class 0
class_weight: {0: 1., 1: 1.}

[koala]
# choose the RNN model e.g. LSTM, GRU
# see https://keras.io/layers/recurrent/
rnn: 'LSTM'

# number of dense (fully connected) layers (int)
n_layers = 2

# number of nodes in each layer (int)
layer_nodes = 60

# bool, if batch normalization should be performed
batch_norm = True

# bool if PReLU activation should be used
activation = 'relu'
# activation = 'PReLU'
# activation = 'ELU'
# activation = 'LeakyReLU'

# batch size (int)
batch_size: 32

# number of epochs (int)
n_epochs: 50

# dropout (float [0,1] or list with length n_layers)
dropout: 0.7

# kernel regularizer (float (0,1))
k_reg: 0.01

# class weight (dictionary, e.g. {0: 1., 1: 50.} this means treat every instance of class 1 as 50 
# instances of class 0
class_weight: {0: 1., 1: 1.}



[calo]
# choose the RNN model e.g. LSTM, GRU
# see https://keras.io/layers/recurrent/
rnn: 'LSTM'

# number of dense (fully connected) layers (int)
n_layers = 4

# number of nodes in each layer (int)
layer_nodes = 100

# bool, if batch normalization should be performed
batch_norm = True

# bool if PReLU activation should be used
activation = 'relu'
# activation = 'LeakyReLU'
# activation = 'ELU'

# batch size (int)
batch_size: 16

# number of epochs (int)
n_epochs: 50

# dropout (float [0,1])
dropout: 0.5

# kernel regularizer (float (0,1))
k_reg: 0.01

# class weight (dictionary, e.g. {0: 1., 1: 50.} this means treat every instance of class 1 as 50 
# instances of class 0
class_weight: {0: 1., 1: 1.}

