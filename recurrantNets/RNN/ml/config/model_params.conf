[NN]
# choose the RNN model e.g. LSTM, GRU
# see https://keras.io/layers/recurrent/
rnn: 'LSTM'

# number of dense (fully connected) layers (int)
n_layers = 3

# number of nodes in each layer (int)
layer_nodes = 1000

# bool, if batch normalization should be performed
batch_norm = True

# bool if PReLU activation should be used
activation = 'relu'
# activation = 'PReLU'
# activation = 'ELU'
# activation = 'LeakyReLU'

# batch size (int)
batch_size: 4

# number of epochs (int)
n_epochs: 300

# dropout (float [0,1])
dropout: 0.7

# kernel regularizer (float (0,1))
k_reg: 0.01

# class weight (dictionary, e.g. {0: 1., 1: 50.} this means treat every instance of class 1 as 50 
# instances of class 0
class_weight: {0: 1., 1: 1.}


# [koala]
# # choose the RNN model e.g. LSTM, GRU
# # see https://keras.io/layers/recurrent/
# rnn: ''

# # number of dense (fully connected) layers (int)
# n_layers = 10

# # number of nodes in each layer (int)
# layer_nodes = 100

# # bool, if batch normalization should be performed
# batch_norm = True

# # bool if PReLU activation should be used
# activation = 'ELU'
# # activation = 'PReLU'

# # batch size (int)
# batch_size: 8

# # number of epochs (int)
# n_epochs: 50

# # dropout (float [0,1])
# dropout: 0.1

# # class weight (dictionary, e.g. {0: 1., 1: 50.} this means treat every instance of class 1 as 50 
# # instances of class 0
# class_weight: {0: 1., 1: 1.}

[koala]
# choose the RNN model e.g. LSTM, GRU
# see https://keras.io/layers/recurrent/
rnn: 'LSTM'

# number of dense (fully connected) layers (int)
n_layers = 4

# number of nodes in each layer (int)
layer_nodes = 100

# bool, if batch normalization should be performed
batch_norm = True

# bool if PReLU activation should be used
activation = 'ELU'
# activation = 'LeakyReLU'
# activation = 'ELU'

# batch size (int)
batch_size: 16

# number of epochs (int)
n_epochs: 50

# dropout (float [0,1])
dropout: 0.5

# kernel regularizer (float (0,1))
k_reg: 0.01


# class weight (dictionary, e.g. {0: 1., 1: 50.} this means treat every instance of class 1 as 50 
# instances of class 0
class_weight: {0: 1., 1: 1.}



[calo]
# choose the RNN model e.g. LSTM, GRU
# see https://keras.io/layers/recurrent/
rnn: 'LSTM'

# number of dense (fully connected) layers (int)
n_layers = 4

# number of nodes in each layer (int)
layer_nodes = 100

# bool, if batch normalization should be performed
batch_norm = True

# bool if PReLU activation should be used
activation = 'relu'
# activation = 'LeakyReLU'
# activation = 'ELU'

# batch size (int)
batch_size: 16

# number of epochs (int)
n_epochs: 50

# dropout (float [0,1])
dropout: 0.5

# kernel regularizer (float (0,1))
k_reg: 0.01

# class weight (dictionary, e.g. {0: 1., 1: 50.} this means treat every instance of class 1 as 50 
# instances of class 0
class_weight: {0: 1., 1: 1.}

