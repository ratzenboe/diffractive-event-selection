[SimpleGrid]

# choose the RNN model e.g. LSTM, GRU
# see https://keras.io/layers/recurrent/
rnn: 'LSTM'

# number of dense (fully connected) layers (int)
n_layers = 5

# number of nodes in each layer (int)
layer_nodes = 100

# bool, if batch normalization should be performed
batch_norm = False

# bool if PReLU activation should be used
activation = 'relu'
# activation = 'PReLU'

# batch size (int)
batch_size: 10

# number of epochs (int)
n_epochs: 20

# dropout (float [0,1])
dropout: 0.0

# class weight (dictionary, e.g. {0: 1., 1: 50.} this means treat every instance of class 1 as 50 
# instances of class 0
class_weight: {0: 1., 1: 10.}


[P8own]
rnn: ''

[FullGrid]
rnn: ''


[NN]
# choose the RNN model e.g. LSTM, GRU
# see https://keras.io/layers/recurrent/
rnn: ''

# number of dense (fully connected) layers (int)
n_layers = 3

# number of nodes in each layer (int)
layer_nodes = 100

# bool, if batch normalization should be performed
batch_norm = False

# bool if PReLU activation should be used
activation = 'relu'
# activation = 'PReLU'

# batch size (int)
batch_size: 10

# number of epochs (int)
n_epochs: 10

# dropout (float [0,1])
dropout: 0.0

# class weight (dictionary, e.g. {0: 1., 1: 50.} this means treat every instance of class 1 as 50 
# instances of class 0
class_weight: {0: 1., 1: 10.}


[anomaly]

# list of layer-nodes in the densly connected layers
n_layers = [30, 20, 10, 5, 10, 20, 30]

# bool, if batch normalization should be performed
batch_norm = False

# bool if PReLU activation should be used
# activation = 'relu'
activation = 'PReLU'

# batch size (int)
batch_size: 32

# number of epochs (int)
n_epochs: 100

# dropout (float [0,1])
dropout: 0.1

# ##########################################################
# the next variables are not needed but right now the program
# is to unflexible to leave them in 
layer_nodes = 100
rnn: ''
class_weight: {0: 1., 1: 10.}
